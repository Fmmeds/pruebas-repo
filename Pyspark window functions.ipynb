{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accomplished-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creamos SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('Window_functions') \\\n",
    "        .master('local[*]') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "understood-stephen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Window_functions</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5751adc7d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See info about SparkSession and link to Spark UI\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-plymouth",
   "metadata": {},
   "source": [
    "Una función ventana es una función agregada aplicada a una partición o subconjunto del resultado de una consulta, devuelve un valor por cada fila del resultado de una consulta.\n",
    "\n",
    "Para trabajar con ventanas se pueden utilizar las funciones agregadas normales y específicas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-klein",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Ejemplo Sparkbyexamples.\n",
    "https://sparkbyexamples.com/pyspark/pyspark-window-functions/\n",
    "\n",
    "Las funciones de ventana se utilizan para calcular resultados como rankings o numero de filas en una ventana (rango de filas de entrada). Son utiles cuando necesitamos realizar operaciones de agregacion en un marco de ventana especifico en las columnas de un DF.\n",
    "\n",
    "\n",
    "Las funciones de la ventana PySpark operan en un grupo de filas (como marco, partición) y devuelven un valor único para cada fila de entrada. PySpark SQL admite tres tipos de funciones de ventana:\n",
    "- funciones de rankings\n",
    "- funciones analiticas\n",
    "- funciones de agregacion\n",
    "\n",
    "Para el caso de las funciones de rankings (rank,dense_rank) y analytics(lag,lead,cum_dist) será necesario usar la clausula order pasanlode la ventana creada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "destroyed-patient",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Create pyspark df ##\n",
    "\n",
    "schema = \"\"\" employee_name STRING, \n",
    "             department STRING, \n",
    "             salary INT  \"\"\"\n",
    "\n",
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    "\n",
    "df = spark.createDataFrame(data = simpleData, schema = schema)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-conspiracy",
   "metadata": {},
   "source": [
    "# 1. PySpark window Rankings functions\n",
    "Primero creamos nuestra ventana con la el metodo window, pasandole partittionby con la columna a particionar y orderby con la columna que ordenará la partición.\n",
    "\n",
    "\n",
    "**row_number()** se usa para dar el número de fila o registro comenzando desde 1 hasta el resultado de cada partición de ventana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "incredible-asthma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|James        |Sales     |3000  |1         |\n",
      "|James        |Sales     |3000  |2         |\n",
      "|Robert       |Sales     |4100  |3         |\n",
      "|Saif         |Sales     |4100  |4         |\n",
      "|Michael      |Sales     |4600  |5         |\n",
      "|Maria        |Finance   |3000  |1         |\n",
      "|Scott        |Finance   |3300  |2         |\n",
      "|Jen          |Finance   |3900  |3         |\n",
      "|Kumar        |Marketing |2000  |1         |\n",
      "|Jeff         |Marketing |3000  |2         |\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, desc\n",
    "\n",
    "# Create window (parttition by department and order by salary asc)\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "# Create column row_number using window with row_number() function\n",
    "df2 = df.withColumn(\"row_number\", row_number().over(windowSpec))\n",
    "df2.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-orange",
   "metadata": {},
   "source": [
    "**rank()** obtiene un ranking/clasificacion de filas dentro de una particion de ventana ordenada. A los empates se les asigna el mismo rango, y se omiten los siguientes rangos.\n",
    "\n",
    "ej si tiene 3 elementos en el rango 2, el siguiente rango en la lista sería el 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-parallel",
   "metadata": {},
   "source": [
    "Si nos fijamos en el ejemplo hay 3 rankigs, uno para cada departamento.\n",
    "\n",
    "Nuestra ventana esta particionada por departamento y ordenada por salario en orden ascendente.\n",
    "\n",
    "rank() en caso de empates, por ejemplo las 2 primeras filas de salary, les da a las dos el mismo rango y omite el siguiente, saltandose el 2, lo mimo ocurre con las filas de Robert y Saif, empatan por salario de 4100 y se salta el 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "general-lafayette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|   1|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|       Robert|     Sales|  4100|   3|\n",
      "|         Saif|     Sales|  4100|   3|\n",
      "|      Michael|     Sales|  4600|   5|\n",
      "|        Maria|   Finance|  3000|   1|\n",
      "|        Scott|   Finance|  3300|   2|\n",
      "|          Jen|   Finance|  3900|   3|\n",
      "|        Kumar| Marketing|  2000|   1|\n",
      "|         Jeff| Marketing|  3000|   2|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "\n",
    "df.withColumn(\"rank\", rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-format",
   "metadata": {},
   "source": [
    "**dense_rank()** obtiene un ranking/clasificacion de filas dentro de una particion de ventana ordenada.  La diferencia con rank es los rankings son consecutivos, no omitiendo ningun rango si hay empates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-rescue",
   "metadata": {},
   "source": [
    "En este caso vemos que en el empate entre los dos James, les da a los 2 el primer rango y pasa al 2, ocurriendo lo mismo con Robert y Saif dandole el 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "classified-budapest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|dense_rank|\n",
      "+-------------+----------+------+----------+\n",
      "|        James|     Sales|  3000|         1|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|       Robert|     Sales|  4100|         2|\n",
      "|         Saif|     Sales|  4100|         2|\n",
      "|      Michael|     Sales|  4600|         3|\n",
      "|        Maria|   Finance|  3000|         1|\n",
      "|        Scott|   Finance|  3300|         2|\n",
      "|          Jen|   Finance|  3900|         3|\n",
      "|        Kumar| Marketing|  2000|         1|\n",
      "|         Jeff| Marketing|  3000|         2|\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    "\n",
    "df.withColumn(\"dense_rank\", dense_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-millennium",
   "metadata": {},
   "source": [
    "**percent_rank()** devuelve el percentil de las filas o registros dentro de una particion de ventana ordenada.\n",
    "\n",
    "En este caso podemos ver que asigna el mismo percentil en caso de empate y sin saltarse los rangos de percentil consecutivos.\n",
    "\n",
    "El percentil es una medida de posición usada en estadística que indica, una vez ordenados los datos de menor a mayor, el valor de la variable por debajo del cual se encuentra un porcentaje dado de observaciones en un grupo. Por ejemplo, el percentil 20.º es el valor bajo el cual se encuentran el 20 por ciento de las observaciones. Él y el 80% restante son mayores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "handled-perth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------+\n",
      "|employee_name|department|salary|percent_rank|\n",
      "+-------------+----------+------+------------+\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|       Robert|     Sales|  4100|         0.5|\n",
      "|         Saif|     Sales|  4100|         0.5|\n",
      "|      Michael|     Sales|  4600|         1.0|\n",
      "|        Maria|   Finance|  3000|         0.0|\n",
      "|        Scott|   Finance|  3300|         0.5|\n",
      "|          Jen|   Finance|  3900|         1.0|\n",
      "|        Kumar| Marketing|  2000|         0.0|\n",
      "|         Jeff| Marketing|  3000|         1.0|\n",
      "+-------------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import percent_rank\n",
    "\n",
    "df.withColumn(\"percent_rank\", percent_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-occurrence",
   "metadata": {},
   "source": [
    "**ntile()** devielve el ranking en base al argumento ntile dado. \n",
    "\n",
    "En el ejemplo le pasamos el 2 como argumento a ntile para que devuelva el ranking entre 2 valores (1 y 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "floating-reminder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----+\n",
      "|employee_name|department|salary|ntile|\n",
      "+-------------+----------+------+-----+\n",
      "|        James|     Sales|  3000|    1|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|       Robert|     Sales|  4100|    1|\n",
      "|         Saif|     Sales|  4100|    2|\n",
      "|      Michael|     Sales|  4600|    2|\n",
      "|        Maria|   Finance|  3000|    1|\n",
      "|        Scott|   Finance|  3300|    1|\n",
      "|          Jen|   Finance|  3900|    2|\n",
      "|        Kumar| Marketing|  2000|    1|\n",
      "|         Jeff| Marketing|  3000|    2|\n",
      "+-------------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import ntile\n",
    "\n",
    "df.withColumn(\"ntile\", ntile(2).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-logistics",
   "metadata": {},
   "source": [
    "# 2. PySpark Window Analytic functions\n",
    "\n",
    "**cume_dist()** devuelve la una distribucion acumulada de valores en una particion de ventana ordenada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "photographic-arbor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------------+\n",
      "|employee_name|department|salary|         cume_dist|\n",
      "+-------------+----------+------+------------------+\n",
      "|        James|     Sales|  3000|               0.4|\n",
      "|        James|     Sales|  3000|               0.4|\n",
      "|       Robert|     Sales|  4100|               0.8|\n",
      "|         Saif|     Sales|  4100|               0.8|\n",
      "|      Michael|     Sales|  4600|               1.0|\n",
      "|        Maria|   Finance|  3000|0.3333333333333333|\n",
      "|        Scott|   Finance|  3300|0.6666666666666666|\n",
      "|          Jen|   Finance|  3900|               1.0|\n",
      "|        Kumar| Marketing|  2000|               0.5|\n",
      "|         Jeff| Marketing|  3000|               1.0|\n",
      "+-------------+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import cume_dist\n",
    "\n",
    "df.withColumn(\"cume_dist\", cume_dist().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-waste",
   "metadata": {},
   "source": [
    "**lag(col, offset)** utiliza una columna numerica para, en base al offset (valor numerico) desplazar los valores de la fila hacia atras.\n",
    "\n",
    "En el ejemplo que, àra el departamento Sales, desplazamos los valores hacia atras de la columna salary 1 vez, por lo que Michael pasa a tener el salario que antes tenia Saif (de 4600 a 4100), Saif el salario que antes tenia Robert (de 4100 a 3000), y asi sucesivamente hasta James, que como es el primero no se puede desplazar y se le da un valor nulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "western-angola",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary| lag|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|null|\n",
      "|        James|     Sales|  3000|3000|\n",
      "|       Robert|     Sales|  4100|3000|\n",
      "|         Saif|     Sales|  4100|4100|\n",
      "|      Michael|     Sales|  4600|4100|\n",
      "|        Maria|   Finance|  3000|null|\n",
      "|        Scott|   Finance|  3300|3000|\n",
      "|          Jen|   Finance|  3900|3300|\n",
      "|        Kumar| Marketing|  2000|null|\n",
      "|         Jeff| Marketing|  3000|2000|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag  \n",
    "\n",
    "df.withColumn(\"lag\",lag(\"salary\",1).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-regulation",
   "metadata": {},
   "source": [
    "**lead(col, offset)** utiliza una columna numerica para, en base al offset (valor numerico) desplazar los valores de la fila hacia adelante.\n",
    "\n",
    "En el ejemplo podemos ver que, para el departamento Sales, desplazamos los registros de la columna salary hacia adelante 1 vez. Por lo que, James pasaria a tener el salario de James (de 3000 a 3000), James pasaria a tener el salario de rober (de 3000 a 4100) y asi sucesivamente hasta Michael, que como no puede avanzar se le da un valor nulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "recorded-tuition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|lead|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|3000|\n",
      "|        James|     Sales|  3000|4100|\n",
      "|       Robert|     Sales|  4100|4100|\n",
      "|         Saif|     Sales|  4100|4600|\n",
      "|      Michael|     Sales|  4600|null|\n",
      "|        Maria|   Finance|  3000|3300|\n",
      "|        Scott|   Finance|  3300|3900|\n",
      "|          Jen|   Finance|  3900|null|\n",
      "|        Kumar| Marketing|  2000|3000|\n",
      "|         Jeff| Marketing|  3000|null|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead    \n",
    "\n",
    "df.withColumn(\"lead\",lead(\"salary\",1).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-surface",
   "metadata": {},
   "source": [
    "# 4. PySpark Window Analytic functions\n",
    "\n",
    "De la misma manera, se puede realizar **operaciones de agregacion como maximos, minimos, totales, medias, etc por ventana** (en el caso del ejemplo recordemos que la ventana esta particionada por departamento y ordenada por salario en orden ascendente). \n",
    "\n",
    "Se hace de la misma forma, le pasamos columna a la funcion de agregacion y le añadimos la clausula over con la ventana creada previamente con la funcion windown. Cuando trabajamos con funciones agregadas dentro de una ventaan, no necesitamos la clausula orderby para ordenar la ventana.\n",
    "\n",
    "Destacamos del ejemplo que utilizamos la funcion de ventana row_number para obtener el numero de registro en la ventana \n",
    "\".withColumn(\"row\",row_number().over(windowSpec)\" y añadimos un where para que no solo muestre 1 vez el departamento y no muestre por cada registro \".where(col(\"row\")==1)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "linear-christianity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+----+----+\n",
      "|department|   avg|  sum| min| max|\n",
      "+----------+------+-----+----+----+\n",
      "|     Sales|3760.0|18800|3000|4600|\n",
      "|   Finance|3400.0|10200|3000|3900|\n",
      "| Marketing|2500.0| 5000|2000|3000|\n",
      "+----------+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number\n",
    "\n",
    "# Create window partitioned bt department column\n",
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "\n",
    "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
    "  .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
